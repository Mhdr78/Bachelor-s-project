{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Post-Process of Crawled datas"
      ],
      "metadata": {
        "id": "TRMTQfN4QBB0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g-fnVinCiiS"
      },
      "source": [
        "## import tweeta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CHmsXptCnol",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32357074-711d-4790-8571-a5e55170800e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1dDOuGdESZ118ON62IBUuRXCdZb9IJ_zh\n",
            "To: /content/tweeta-0.2.1.rar\n",
            "\r  0% 0.00/28.9k [00:00<?, ?B/s]\r100% 28.9k/28.9k [00:00<00:00, 23.2MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/u/0/uc?id=1dDOuGdESZ118ON62IBUuRXCdZb9IJ_zh&export=download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZhOz0T5CykN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d1a595a-68bc-4794-adca-c6d6761845b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from tweeta-0.2.1.rar\n",
            "\n",
            "Creating    docs                                                      OK\n",
            "Extracting  docs/Makefile                                                \b\b\b\b  1%\b\b\b\b\b  OK \n",
            "Creating    docs/source                                               OK\n",
            "Extracting  docs/source/conf.py                                          \b\b\b\b  9%\b\b\b\b\b  OK \n",
            "Extracting  docs/source/index.rst                                        \b\b\b\b 10%\b\b\b\b\b  OK \n",
            "Creating    tweeta                                                    OK\n",
            "Extracting  tweeta/constants.py                                          \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  tweeta/exceptions.py                                         \b\b\b\b 11%\b\b\b\b\b  OK \n",
            "Extracting  tweeta/text.py                                               \b\b\b\b 27%\b\b\b\b\b  OK \n",
            "Extracting  tweeta/tweet.py                                              \b\b\b\b 36%\b\b\b\b\b  OK \n",
            "Extracting  tweeta/__init__.py                                           \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Extracting  HISTORY.md                                                   \b\b\b\b 37%\b\b\b\b\b  OK \n",
            "Extracting  LICENSE                                                      \b\b\b\b 39%\b\b\b\b\b  OK \n",
            "Extracting  MANIFEST.in                                                  \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Extracting  README.md                                                    \b\b\b\b 40%\b\b\b\b\b  OK \n",
            "Extracting  requirements.txt                                             \b\b\b\b 41%\b\b\b\b\b  OK \n",
            "Extracting  setup.py                                                     \b\b\b\b 45%\b\b\b\b\b  OK \n",
            "Extracting  setup.py.bak                                                 \b\b\b\b 49%\b\b\b\b\b  OK \n",
            "Extracting  test                                                         \b\b\b\b 53%\b\b\b\b\b  OK \n",
            "Extracting  tweeta_test.ipynb                                            \b\b\b\b 98%\b\b\b\b\b  OK \n",
            "All OK\n"
          ]
        }
      ],
      "source": [
        "!unrar x tweeta-0.2.1.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj3FNmAxDBkv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6c2c87a-6723-438d-935d-3be49fc7cb31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating tweeta.egg-info\n",
            "writing tweeta.egg-info/PKG-INFO\n",
            "writing dependency_links to tweeta.egg-info/dependency_links.txt\n",
            "writing requirements to tweeta.egg-info/requires.txt\n",
            "writing top-level names to tweeta.egg-info/top_level.txt\n",
            "writing manifest file 'tweeta.egg-info/SOURCES.txt'\n",
            "reading manifest file 'tweeta.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'tweeta.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/tweeta\n",
            "copying tweeta/exceptions.py -> build/lib/tweeta\n",
            "copying tweeta/tweet.py -> build/lib/tweeta\n",
            "copying tweeta/constants.py -> build/lib/tweeta\n",
            "copying tweeta/__init__.py -> build/lib/tweeta\n",
            "copying tweeta/text.py -> build/lib/tweeta\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/tweeta\n",
            "copying build/lib/tweeta/exceptions.py -> build/bdist.linux-x86_64/egg/tweeta\n",
            "copying build/lib/tweeta/tweet.py -> build/bdist.linux-x86_64/egg/tweeta\n",
            "copying build/lib/tweeta/constants.py -> build/bdist.linux-x86_64/egg/tweeta\n",
            "copying build/lib/tweeta/__init__.py -> build/bdist.linux-x86_64/egg/tweeta\n",
            "copying build/lib/tweeta/text.py -> build/bdist.linux-x86_64/egg/tweeta\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tweeta/exceptions.py to exceptions.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tweeta/tweet.py to tweet.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tweeta/constants.py to constants.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tweeta/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/tweeta/text.py to text.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tweeta.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tweeta.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tweeta.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tweeta.egg-info/requires.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying tweeta.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/tweeta-0.2.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing tweeta-0.2.1-py3.7.egg\n",
            "Copying tweeta-0.2.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding tweeta 0.2.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/tweeta-0.2.1-py3.7.egg\n",
            "Processing dependencies for tweeta==0.2.1\n",
            "Searching for emoji>=0.4.5\n",
            "Reading https://pypi.org/simple/emoji/\n",
            "Downloading https://files.pythonhosted.org/packages/05/d9/6592e0879ba2e3e0cfc3bd381e6c9f7a71e0141c63f32b878be489f23558/emoji-2.1.0.tar.gz#sha256=56a8c5e906c11694eb7694b78e5452d745030869b3945f6306a8151ff5cdbc39\n",
            "Best match: emoji 2.1.0\n",
            "Processing emoji-2.1.0.tar.gz\n",
            "Writing /tmp/easy_install-y2kbj791/emoji-2.1.0/setup.cfg\n",
            "Running emoji-2.1.0/setup.py -q bdist_egg --dist-dir /tmp/easy_install-y2kbj791/emoji-2.1.0/egg-dist-tmp-tsi84su2\n",
            "Moving emoji-2.1.0-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding emoji 2.1.0 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/emoji-2.1.0-py3.7.egg\n",
            "Searching for langid>=1.1.6\n",
            "Reading https://pypi.org/simple/langid/\n",
            "Downloading https://files.pythonhosted.org/packages/ea/4c/0fb7d900d3b0b9c8703be316fbddffecdab23c64e1b46c7a83561d78bd43/langid-1.1.6.tar.gz#sha256=044bcae1912dab85c33d8e98f2811b8f4ff1213e5e9a9e9510137b84da2cb293\n",
            "Best match: langid 1.1.6\n",
            "Processing langid-1.1.6.tar.gz\n",
            "Writing /tmp/easy_install-osbvyh0_/langid-1.1.6/setup.cfg\n",
            "Running langid-1.1.6/setup.py -q bdist_egg --dist-dir /tmp/easy_install-osbvyh0_/langid-1.1.6/egg-dist-tmp-v0vdopkw\n",
            "  File \"build/bdist.linux-x86_64/egg/langid/train/NBtrain.py\", line 167\n",
            "    print \"learning NB parameters on {} items\".format(len(items))\n",
            "                                             ^\n",
            "SyntaxError: invalid syntax\n",
            "\n",
            "  File \"build/bdist.linux-x86_64/egg/langid/train/DFfeatureselect.py\", line 92\n",
            "    print \"processed bucket (%d/%d) [%d keys]\" % (i+1, len(bucketlist), keycount)\n",
            "                                             ^\n",
            "SyntaxError: invalid syntax\n",
            "\n",
            "  File \"build/bdist.linux-x86_64/egg/langid/train/BLweight.py\", line 84\n",
            "    print \"languages({1}): {0}\".format(langs, len(langs))\n",
            "                              ^\n",
            "SyntaxError: invalid syntax\n",
            "\n",
            "Sorry: TabError: inconsistent use of tabs and spaces in indentation (IGweight.py, line 101)\n",
            "  File \"build/bdist.linux-x86_64/egg/langid/train/index.py\", line 253\n",
            "    print \"corpus path:\", args.corpus\n",
            "                       ^\n",
            "SyntaxError: Missing parentheses in call to 'print'. Did you mean print(\"corpus path:\", args.corpus)?\n",
            "\n",
            "  File \"build/bdist.linux-x86_64/egg/langid/train/tokenize.py\", line 239\n",
            "    print \"chunk size: {0} ({1} chunks)\".format(chunk_size, chunk_count)\n",
            "                                       ^\n",
            "SyntaxError: invalid syntax\n",
            "\n",
            "  File \"build/bdist.linux-x86_64/egg/langid/train/scanner.py\", line 193\n",
            "    print \"building scanner\"\n",
            "                           ^\n",
            "SyntaxError: Missing parentheses in call to 'print'. Did you mean print(\"building scanner\")?\n",
            "\n",
            "  File \"build/bdist.linux-x86_64/egg/langid/train/LDfeatureselect.py\", line 94\n",
            "    print \"model path:\", args.model\n",
            "                      ^\n",
            "SyntaxError: Missing parentheses in call to 'print'. Did you mean print(\"model path:\", args.model)?\n",
            "\n",
            "  File \"build/bdist.linux-x86_64/egg/langid/train/train.py\", line 105\n",
            "    print \"corpus path:\", args.corpus\n",
            "                       ^\n",
            "SyntaxError: Missing parentheses in call to 'print'. Did you mean print(\"corpus path:\", args.corpus)?\n",
            "\n",
            "creating /usr/local/lib/python3.7/dist-packages/langid-1.1.6-py3.7.egg\n",
            "Extracting langid-1.1.6-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/langid-1.1.6-py3.7.egg/langid/train/BLweight.py\", line 84\n",
            "    print \"languages({1}): {0}\".format(langs, len(langs))\n",
            "                              ^\n",
            "SyntaxError: invalid syntax\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/langid-1.1.6-py3.7.egg/langid/train/DFfeatureselect.py\", line 92\n",
            "    print \"processed bucket (%d/%d) [%d keys]\" % (i+1, len(bucketlist), keycount)\n",
            "                                             ^\n",
            "SyntaxError: invalid syntax\n",
            "\n",
            "Sorry: TabError: inconsistent use of tabs and spaces in indentation (IGweight.py, line 101)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/langid-1.1.6-py3.7.egg/langid/train/LDfeatureselect.py\", line 94\n",
            "    print \"model path:\", args.model\n",
            "                      ^\n",
            "SyntaxError: Missing parentheses in call to 'print'. Did you mean print(\"model path:\", args.model)?\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/langid-1.1.6-py3.7.egg/langid/train/NBtrain.py\", line 167\n",
            "    print \"learning NB parameters on {} items\".format(len(items))\n",
            "                                             ^\n",
            "SyntaxError: invalid syntax\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/langid-1.1.6-py3.7.egg/langid/train/index.py\", line 253\n",
            "    print \"corpus path:\", args.corpus\n",
            "                       ^\n",
            "SyntaxError: Missing parentheses in call to 'print'. Did you mean print(\"corpus path:\", args.corpus)?\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/langid-1.1.6-py3.7.egg/langid/train/scanner.py\", line 193\n",
            "    print \"building scanner\"\n",
            "                           ^\n",
            "SyntaxError: Missing parentheses in call to 'print'. Did you mean print(\"building scanner\")?\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/langid-1.1.6-py3.7.egg/langid/train/tokenize.py\", line 239\n",
            "    print \"chunk size: {0} ({1} chunks)\".format(chunk_size, chunk_count)\n",
            "                                       ^\n",
            "SyntaxError: invalid syntax\n",
            "\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/langid-1.1.6-py3.7.egg/langid/train/train.py\", line 105\n",
            "    print \"corpus path:\", args.corpus\n",
            "                       ^\n",
            "SyntaxError: Missing parentheses in call to 'print'. Did you mean print(\"corpus path:\", args.corpus)?\n",
            "\n",
            "Adding langid 1.1.6 to easy-install.pth file\n",
            "Installing langid script to /usr/local/bin\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/langid-1.1.6-py3.7.egg\n",
            "Searching for numpy==1.21.6\n",
            "Best match: numpy 1.21.6\n",
            "Adding numpy 1.21.6 to easy-install.pth file\n",
            "Installing f2py script to /usr/local/bin\n",
            "Installing f2py3 script to /usr/local/bin\n",
            "Installing f2py3.7 script to /usr/local/bin\n",
            "\n",
            "Using /usr/local/lib/python3.7/dist-packages\n",
            "Finished processing dependencies for tweeta==0.2.1\n"
          ]
        }
      ],
      "source": [
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSzIVKoGDIvU",
        "outputId": "3e24abe1-f7f5-4814-c4f1-720db5901176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 39.9 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 55.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394488 sha256=cb0cf65db3ae35886b9f480c07a72f2d3ce3fd492a7049c1f01127442b7f64ac\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=153902 sha256=2546a264b1e10f3acbeb3d2a42dc1826225d320e9173aeaee6e67ad6bb3160b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "\u001b[33mWARNING: Skipping ftfy as it is not installed.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Found existing installation: langid 1.1.6\n",
            "Uninstalling langid-1.1.6:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/langid-1.1.6-py3.7.egg\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled langid-1.1.6\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from langid) (1.21.6)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941188 sha256=0457b5aa79c926cc65b2147f18345e0d6bb05a9b1a4b72110143dad0f96b81a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/bb/7f/11e4db39477278161e882eadc46fb558949a28b13470fc74b8\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n",
            "Found existing installation: emoji 2.1.0\n",
            "Uninstalling emoji-2.1.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/emoji-2.1.0-py3.7.egg\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled emoji-2.1.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.1.0.tar.gz (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 4.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-2.1.0-py3-none-any.whl size=212392 sha256=eabc35fdc3ebe6bba7564707d89b8122c0bedce27de33a165caa979dad1076a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/75/99/51c2a119f4cfd3af7b49cc57e4f737bed7e40b348a85d82804\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm\n",
        "!pip uninstall ftfy\n",
        "!pip install ftfy\n",
        "!pip uninstall langid\n",
        "!pip install langid\n",
        "!pip uninstall emoji\n",
        "!pip install emoji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "vf0D0CSwCc5b"
      },
      "outputs": [],
      "source": [
        "import builtins\n",
        "from IPython.lib import deepreload\n",
        "builtins.reload = deepreload.reload"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BxDfqPwUCc5g"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiDv9DdITVFo"
      },
      "outputs": [],
      "source": [
        "import tweeta\n",
        "import emoji\n",
        "import pandas as pd\n",
        "import hazm\n",
        "import re\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z96zal99kGDt"
      },
      "source": [
        "## load crawled datas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VZTOhHIiSwr",
        "outputId": "909d9bdd-f268-493c-f0f5-7b7be2e37bfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1qoydeLHQtF-LHjOdwQXNjO10KHAYHpNR\n",
            "To: /content/hamrah_aval\n",
            "100% 22.3M/22.3M [00:00<00:00, 101MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1ggaHLpe-tEapcAVyiFo_dIOddpIvnu2-\n",
            "To: /content/irancell\n",
            "100% 17.5M/17.5M [00:00<00:00, 66.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1rf5h4gO6MmLYlq-4HpW4kkcLHpmkQ3yq\n",
            "To: /content/rightel\n",
            "100% 1.45M/1.45M [00:00<00:00, 45.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/u/0/uc?id=1qoydeLHQtF-LHjOdwQXNjO10KHAYHpNR&export=download\n",
        "!gdown https://drive.google.com/u/0/uc?id=1ggaHLpe-tEapcAVyiFo_dIOddpIvnu2-&export=download\n",
        "!gdown https://drive.google.com/u/0/uc?id=1rf5h4gO6MmLYlq-4HpW4kkcLHpmkQ3yq&export=download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etvew8YrTqGl"
      },
      "outputs": [],
      "source": [
        "!mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdIT43V7TklM"
      },
      "outputs": [],
      "source": [
        "!mv 'hamrah_aval' 'data/hamrah_aval'\n",
        "!mv 'irancell' 'data/irancell'\n",
        "!mv 'rightel' 'data/rightel'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFs6BjPOT83h"
      },
      "source": [
        "## Postprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vcXVon4T76W"
      },
      "outputs": [],
      "source": [
        "hamrah_aval_file = open('data/hamrah_aval', 'r')\n",
        "irancell_file = open('data/irancell', 'r')\n",
        "rightel_file = open('data/rightel', 'r')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RCufBk4c3m2"
      },
      "source": [
        "### features\n",
        "    # created_time\n",
        "    # user id = tweet.user_id()\n",
        "    # text \n",
        "    # is_retweet = tweet.is_retweet()\n",
        "    # is_quote = tweet.is_quote()\n",
        "    # is_reply = tweet.get('in_reply_to_user_id') != None\n",
        "    # tags = 'hamrah_aval', 'irancell', 'rightel'\n",
        "\n",
        "also we are using [Hazm](https://github.com/sobhe/hazm) library.\n",
        "\n",
        "furthermore we can use the InformalNormalizer instead of Normalizer for better model but the InformalNormalizer takes too much time to process!\n",
        "\n",
        "for using InformalNormalizer we must use the norm_words function too.\n",
        "\n",
        "but even with Normalizer, InformalNormalizer, Stemmer or Lemmatizer, unfortunately Hazm library doesn't fix the informal texts properly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvADQpYsUjRG"
      },
      "outputs": [],
      "source": [
        "normalizer = hazm.Normalizer()\n",
        "exp = re.compile('^.*(همراه من|همراه_من|همراه #من|همراه‌ من).*$')\n",
        "exp2 = re.compile('^.*(همراه اول|همراه_اول|باشگاه|ایرانسل).*$')\n",
        "\n",
        "def create_features(given_file, tag):\n",
        "    l = []\n",
        "\n",
        "    for line in given_file:\n",
        "        json_str = line\n",
        "        tweet = tweeta.TweetaTweet(json_str)\n",
        "\n",
        "        if tweet.is_retweet():\n",
        "            retweet_ = tweet.get('retweeted_status')\n",
        "            retweet = tweeta.TweetaTweet(retweet_)\n",
        "            text = emoji.replace_emoji(retweet.fixed_text(), replace='')\n",
        "            text = tweeta.text.remove_mentions(text)\n",
        "            text = tweeta.text.sanitize_nofunccall(text)\n",
        "        else:\n",
        "            text = emoji.replace_emoji(tweet.fixed_text(), replace='')\n",
        "            text = tweeta.text.remove_mentions(text)\n",
        "            text = tweeta.text.sanitize_nofunccall(text)\n",
        "\n",
        "        text = normalizer.normalize(text)\n",
        "\n",
        "        if exp.match(text) and not exp2.match(text):\n",
        "            continue\n",
        "\n",
        "        #if tweeta.text.lang(text) == 'fa' or tweeta.text.lang(text) == 'ar':\n",
        "        #    l.append({'tag' : tag, 'user_id' : tweet.user_id(), 'is_retweet' : tweet.is_retweet(), 'text' : text})\n",
        "\n",
        "        if tweeta.text.lang(text) != 'en':\n",
        "            l.append({'created_time' : tweet.created_at('%Y-%m-%d %H:%M:%S'), 'user_id' : tweet.user_id(), \\\n",
        "                      'text' : text, 'is_retweet' : tweet.is_retweet(), 'is_quote' : tweet.is_quote(), \\\n",
        "                      'is_reply' : tweet.get('in_reply_to_user_id') != None, 'tags' : {tag}})\n",
        "    \n",
        "    given_file.close()\n",
        "    return l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZ6VXeRId8D8"
      },
      "outputs": [],
      "source": [
        "hamrah_aval_list = create_features(hamrah_aval_file, 'hamrah_aval')\n",
        "irancell_list = create_features(irancell_file, 'irancell')\n",
        "rightel_list = create_features(rightel_file, 'rightel')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def add_tags(list1, list2):\n",
        "    exp_hamrahaval = re.compile('^.*(همراه اول|همراه‌اول|همراه_اول|باشگاه فیروزه).*$')\n",
        "    exp_irancell = re.compile('^.*(ایرانسل|ایرانسل من|ایرانسل_من|ایران سل|ایران‌سل|ایرانسل‌من).*$')\n",
        "    exp_rightel = re.compile('^.*(رایتل|رایتل من|رایتل‌من|رایتل_من).*$')\n",
        "\n",
        "    for i in range(len(list1)):\n",
        "        if exp_hamrahaval.match(list1[i]['text']):\n",
        "            list1[i]['tags'].add('hamrah_aval')\n",
        "        if exp_irancell.match(list1[i]['text']):\n",
        "            list1[i]['tags'].add('irancell')\n",
        "        if exp_rightel.match(list1[i]['text']):\n",
        "            list1[i]['tags'].add('rightel')\n",
        "\n",
        "        list1[i]['tags'] = set(sorted(list1[i]['tags']))\n",
        "\n",
        "        for j in range(len(list2)):\n",
        "            if list1[i]['text'] == list2[j]['text']:\n",
        "                list1[i]['tags'].update(list2[j]['tags'])\n",
        "                list2[j]['tags'].update(list1[i]['tags'])\n",
        "                \n",
        "                list1[i]['tags'] = set(sorted(list1[i]['tags']))\n",
        "                list2[j]['tags'] = set(sorted(list2[j]['tags']))"
      ],
      "metadata": {
        "id": "DawvizFQ1fdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lists = [hamrah_aval_list, irancell_list, rightel_list]"
      ],
      "metadata": {
        "id": "rzWL8RkV34ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(lists)):\n",
        "    for j in range(i + 1, len(lists)):\n",
        "        add_tags(lists[i], lists[j])"
      ],
      "metadata": {
        "id": "Js4lGCK83wnD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erj-M6ZgtjtC"
      },
      "source": [
        "### Convert to csv and save on google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-cWHkf6rmMg"
      },
      "outputs": [],
      "source": [
        "hamrah_aval = pd.DataFrame(hamrah_aval_list)\n",
        "irancell = pd.DataFrame(irancell_list)\n",
        "rightel = pd.DataFrame(rightel_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpZvuWuY86R1"
      },
      "outputs": [],
      "source": [
        "def edit_dataframe(df):\n",
        "    df = df.sort_values(by='created_time', ascending=False, ignore_index=True)\n",
        "    df = df.drop_duplicates(subset=['text'], keep='last')\n",
        "    df = df.dropna(subset=['text'])\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDZ_2EIM9MVt"
      },
      "outputs": [],
      "source": [
        "hamrah_aval = edit_dataframe(hamrah_aval)\n",
        "irancell = edit_dataframe(irancell)\n",
        "rightel = edit_dataframe(rightel)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hamrah_aval.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aoZB1hSe6Z6I",
        "outputId": "f2db9b28-25f6-4da7-c011-04777f519700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          created_time              user_id  \\\n",
              "0  2022-07-19 19:18:27  1355911468902981639   \n",
              "1  2022-07-19 19:13:32            614986877   \n",
              "2  2022-07-19 19:09:10   941220112845164544   \n",
              "3  2022-07-19 19:05:41   822277959176306688   \n",
              "4  2022-07-19 19:00:44  1111363641545236480   \n",
              "\n",
              "                                                text  is_retweet  is_quote  \\\n",
              "0                     هیچ کس تنها نیست … … همراه اول       False     False   \n",
              "1                بلی، ایرانسل از همراه اول بدتره حتی       False     False   \n",
              "2  بسته سه ماهه همراه اول رو دو دوره یعنی شش ماه ...       False     False   \n",
              "3                                 همراه اول هم همینه       False     False   \n",
              "4  بسته‌ی سه گیگ ماهانه‌ی اینترنت همراه اول می‌خر...       False     False   \n",
              "\n",
              "   is_reply                     tags  \n",
              "0      True            {hamrah_aval}  \n",
              "1      True  {hamrah_aval, irancell}  \n",
              "2      True            {hamrah_aval}  \n",
              "3      True            {hamrah_aval}  \n",
              "4     False            {hamrah_aval}  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-59024e03-4d64-4da0-a522-ab56b39429b9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created_time</th>\n",
              "      <th>user_id</th>\n",
              "      <th>text</th>\n",
              "      <th>is_retweet</th>\n",
              "      <th>is_quote</th>\n",
              "      <th>is_reply</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2022-07-19 19:18:27</td>\n",
              "      <td>1355911468902981639</td>\n",
              "      <td>هیچ کس تنها نیست … … همراه اول</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>{hamrah_aval}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2022-07-19 19:13:32</td>\n",
              "      <td>614986877</td>\n",
              "      <td>بلی، ایرانسل از همراه اول بدتره حتی</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>{hamrah_aval, irancell}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2022-07-19 19:09:10</td>\n",
              "      <td>941220112845164544</td>\n",
              "      <td>بسته سه ماهه همراه اول رو دو دوره یعنی شش ماه ...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>{hamrah_aval}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2022-07-19 19:05:41</td>\n",
              "      <td>822277959176306688</td>\n",
              "      <td>همراه اول هم همینه</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>{hamrah_aval}</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2022-07-19 19:00:44</td>\n",
              "      <td>1111363641545236480</td>\n",
              "      <td>بسته‌ی سه گیگ ماهانه‌ی اینترنت همراه اول می‌خر...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>{hamrah_aval}</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59024e03-4d64-4da0-a522-ab56b39429b9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-59024e03-4d64-4da0-a522-ab56b39429b9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-59024e03-4d64-4da0-a522-ab56b39429b9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpkI_J41shp6"
      },
      "outputs": [],
      "source": [
        "hamrah_aval.to_csv('data/hamrah_aval.csv', index=None)\n",
        "irancell.to_csv('data/irancell.csv', index=None)\n",
        "rightel.to_csv('data/rightel.csv', index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaX36Z-QtNGm",
        "outputId": "0876beb3-b582-40de-876c-9192fb95f017"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dWT4LUxwtOPd",
        "outputId": "0691d04f-45ca-45b8-aa7b-17429027c9d8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/gdrive/MyDrive/crawler/csv_files_20220719/rightel.csv'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.copy('data/hamrah_aval.csv', \"/content/gdrive/MyDrive/crawler/csv_files_20220719/hamrah_aval.csv\")\n",
        "shutil.copy('data/irancell.csv', \"/content/gdrive/MyDrive/crawler/csv_files_20220719/irancell.csv\")\n",
        "shutil.copy('data/rightel.csv', \"/content/gdrive/MyDrive/crawler/csv_files_20220719/rightel.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}